{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#sns.set_style(\"darkgrid\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 Plot distribution of the instances in each class and save the graphic in a file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "numOffolds = len(os.listdir('BBC/'))\n",
    "print(numOffolds)\n",
    "all_folds = os.listdir('BBC/')\n",
    "folder_path = all_folds[0]\n",
    "print(folder_path)\n",
    "\n",
    "len([x for x in os.listdir('BBC/'+folder_path)])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "entertainment\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "data_need = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "\n",
    "path = 'BBC/'\n",
    "all_folds = os.listdir(path)\n",
    "\n",
    "for i in range (len(all_folds)):\n",
    "    data_x.append(all_folds[i])\n",
    "    print(f'comput No.{i+1} folder.')\n",
    "    folder_path = all_folds[i]\n",
    "    NumOfFiles = len([x for x in os.listdir(path + folder_path)])\n",
    "    data_y.append(NumOfFiles)\n",
    "    print(f'No.{i+1} has {NumOfFiles} files.')\n",
    "    table = pd.DataFrame({'folder_id': all_folds[i],'file_num':[NumOfFiles]})\n",
    "    data_need.append(table)\n",
    "\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "comput No.1 folder.\n",
      "No.1 has 386 files.\n",
      "comput No.2 folder.\n",
      "No.2 has 510 files.\n",
      "comput No.3 folder.\n",
      "No.3 has 511 files.\n",
      "comput No.4 folder.\n",
      "No.4 has 417 files.\n",
      "comput No.5 folder.\n",
      "No.5 has 401 files.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "data_final = pd.concat(data_need, axis=0)\n",
    "print(data_final)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       folder_id  file_num\n",
      "0  entertainment       386\n",
      "0       business       510\n",
      "0          sport       511\n",
      "0       politics       417\n",
      "0           tech       401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "x_np = np.array(data_x)\n",
    "print(x_np)\n",
    "y_np = np.array(data_y)\n",
    "print(y_np)\n",
    "plt.title(\"Distribution of the instances in each class\")\n",
    "plt.xlabel(\"class name\")\n",
    "plt.ylabel(\"number of instances\")\n",
    "plt.bar(x_np,y_np)\n",
    "plt.savefig('BBC-Distribution.pdf')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['entertainment' 'business' 'sport' 'politics' 'tech']\n",
      "[386 510 511 417 401]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# use load_files to read the file structure and assign the category name to each file\n",
    "corpus_data = load_files(\"BBC\",encoding=\"latin1\")\n",
    "corpus_data_data = corpus_data.data\n",
    "# print(corpus_data_data)\n",
    "corpus_category = corpus_data.target\n",
    "print(corpus_category)\n",
    "corpus_category_names = corpus_data.target_names\n",
    "print(corpus_category_names)\n",
    "\n",
    "\n",
    " \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 4 2 ... 1 1 3]\n",
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4 Pre-process the dataset to have the features ready to be used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "#create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "#tokenize and build vocab\n",
    "vectorizer.fit(corpus_data_data)\n",
    "#summarize\n",
    "vocabulary_corpus = vectorizer.get_feature_names()\n",
    "# print(vocabulary_corpus)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "\n",
    "# Encode the dataset to document-term matrix\n",
    "vector = vectorizer.transform(corpus_data_data)\n",
    "print(vector.shape)\n",
    "print(vector.toarray().sum())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29421\n",
      "(2225, 29421)\n",
      "836357\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]]\n",
      "9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5 Split the dataset int 80% for training 20% for testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# split the dataset into 80% for training and 20% for testing\n",
    "X = corpus_data_data\n",
    "y = corpus_category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state= None)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# countVectorizer\n",
    "vec = CountVectorizer()\n",
    "# fit the vectorizer on the training dataset\n",
    "vec.fit(X_train)\n",
    "print(len(vec.get_feature_names()))\n",
    "vocab = vec.get_feature_names()\n",
    "# print(vocab)\n",
    "X_train_transformed = vec.transform(X_train)\n",
    "# print(X_train_transformed)\n",
    "# convert training set to document-term matrix\n",
    "X_train_feat = X_train_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_train_feat,columns= [vec.get_feature_names()])\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26739\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>0051</th>\n",
       "      <th>...</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 26739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00 000 000bn 000m 000th 001 001and 001st 004 0051  ... zooms zooropa  \\\n",
       "0     0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1     0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "2     0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "3     0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "4     0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "...  ..  ..   ...  ...   ...  ..    ...   ...  ..  ...  ...   ...     ...   \n",
       "1775  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1776  0   1     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1777  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1778  0   1     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1779  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "\n",
       "     zornotza zorro zubair zuluaga zurich zutons zvonareva zvyagintsev  \n",
       "0           0     0      0       0      0      0         0           0  \n",
       "1           0     0      0       0      0      0         0           0  \n",
       "2           0     0      0       0      0      0         0           0  \n",
       "3           0     0      0       0      0      0         0           0  \n",
       "4           0     0      0       0      0      0         0           0  \n",
       "...       ...   ...    ...     ...    ...    ...       ...         ...  \n",
       "1775        0     0      0       0      0      0         0           0  \n",
       "1776        0     0      0       0      0      0         0           0  \n",
       "1777        0     0      0       0      0      0         0           0  \n",
       "1778        0     0      0       0      0      0         0           0  \n",
       "1779        0     0      0       0      0      0         0           0  \n",
       "\n",
       "[1780 rows x 26739 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# for test data\n",
    "X_test_transformed = vec.transform(X_test)\n",
    "# X_test_transformed\n",
    "# print(X_test_transformed)\n",
    "# convert testing set to document-term matrix\n",
    "X_test_feat = X_test_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_test_feat, columns= [vec.get_feature_names()])\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>0051</th>\n",
       "      <th>...</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 26739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00 000 000bn 000m 000th 001 001and 001st 004 0051  ... zooms zooropa  \\\n",
       "0    0   1     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "1    0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "2    0   1     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "3    0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "4    0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "..  ..  ..   ...  ...   ...  ..    ...   ...  ..  ...  ...   ...     ...   \n",
       "440  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "441  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "442  0   2     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "443  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "444  0   0     0    0     0   0      0     0   0    0  ...     0       0   \n",
       "\n",
       "    zornotza zorro zubair zuluaga zurich zutons zvonareva zvyagintsev  \n",
       "0          0     0      0       0      0      0         0           0  \n",
       "1          0     0      0       0      0      0         0           0  \n",
       "2          0     0      0       0      0      0         0           0  \n",
       "3          0     0      0       0      0      0         0           0  \n",
       "4          0     0      0       0      0      0         0           0  \n",
       "..       ...   ...    ...     ...    ...    ...       ...         ...  \n",
       "440        0     0      0       0      0      0         0           0  \n",
       "441        0     0      0       0      0      0         0           0  \n",
       "442        0     0      0       0      0      0         0           0  \n",
       "443        0     0      0       0      0      0         0           0  \n",
       "444        0     0      0       0      0      0         0           0  \n",
       "\n",
       "[445 rows x 26739 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 train a multinomial Naive Bayes Classifier on the training set using the default parameters and evaluate it on the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "f=open('bbc-performance.txt','w')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# multinomial Naive Bayes\n",
    "print(\"--------------------------------MultinomialNB default values, try 1-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \", file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class), file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \", file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "\n",
    "# index = nb.predict(vec.transform(['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']))\n",
    "\n",
    "# def type_check(i):\n",
    "#     if i == 0:\n",
    "#         print('business')\n",
    "#     elif i == 1:\n",
    "#         print('entertainment')\n",
    "#     elif i == 2:\n",
    "#         print('politics')\n",
    "#     elif i == 3:\n",
    "#         print('sport')\n",
    "#     elif i == 4:\n",
    "#         print('tech')\n",
    "# type_check(index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "\n",
    "# counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "print(\"love:\\n\",file=f)\n",
    "favorite_index_1 = vocab.index('love')\n",
    "business = nb.feature_log_prob_[0][favorite_index_1]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_1]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_1]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_1]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_1]\n",
    "table_k_1 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_1,file=f)\n",
    "print(\"key:\\n\",file=f)\n",
    "favorite_index_2 = vocab.index('key')\n",
    "business = nb.feature_log_prob_[0][favorite_index_2]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_2]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_2]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_2]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_2]\n",
    "table_k_2 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_2,file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB default value, try 2-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "print(\"love:\\n\",file=f)\n",
    "favorite_index_1 = vocab.index('love')\n",
    "business = nb.feature_log_prob_[0][favorite_index_1]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_1]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_1]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_1]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_1]\n",
    "table_k_1 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_1,file=f)\n",
    "print(\"key:\\n\",file=f)\n",
    "favorite_index_2 = vocab.index('key')\n",
    "business = nb.feature_log_prob_[0][favorite_index_2]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_2]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_2]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_2]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_2]\n",
    "table_k_2 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_2,file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. smooth value to 0.0001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB smooth valve to 0.0001-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=0.0001)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "print(\"love:\\n\",file=f)\n",
    "favorite_index_1 = vocab.index('love')\n",
    "business = nb.feature_log_prob_[0][favorite_index_1]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_1]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_1]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_1]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_1]\n",
    "table_k_1 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_1,file=f)\n",
    "print(\"key:\\n\",file=f)\n",
    "favorite_index_2 = vocab.index('key')\n",
    "business = nb.feature_log_prob_[0][favorite_index_2]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_2]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_2]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_2]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_2]\n",
    "table_k_2 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_2,file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Redo steps 6 and 7, but this time change the smooth value to 0.9. Appendthe results at the end of bbc-performance.tx"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB smooth 0.9-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=0.9)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "print(\"love:\\n\",file=f)\n",
    "favorite_index_1 = vocab.index('love')\n",
    "business = nb.feature_log_prob_[0][favorite_index_1]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_1]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_1]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_1]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_1]\n",
    "table_k_1 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_1,file=f)\n",
    "print(\"key:\\n\",file=f)\n",
    "favorite_index_2 = vocab.index('key')\n",
    "business = nb.feature_log_prob_[0][favorite_index_2]\n",
    "entertainment = nb.feature_log_prob_[1][favorite_index_2]\n",
    "politics = nb.feature_log_prob_[2][favorite_index_2]\n",
    "sport = nb.feature_log_prob_[3][favorite_index_2]\n",
    "tech = nb.feature_log_prob_[4][favorite_index_2]\n",
    "table_k_2 = pd.DataFrame(np.array([[business,entertainment,politics,sport,tech]]),columns=['business','entertainment','politics','sport','tech'])\n",
    "print(table_k_2,file=f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8fce6691405556ffb3062e1861de211962c440fc65c8599586c6206ef07fdf2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}